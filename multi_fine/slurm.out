#################################
########  Folder 0  ############
device:  cuda:0
Namespace(batchsize=32, beta1=0.5, dataroot='../../../dataset', dataset='ecg', device='gpu', folder=0, gpu_ids=[2], isize=128, istest=False, lr=0.0001, model='beatgan', n_aug=0, name='beatgan/ecg', nc=1, ndf=54, ndfs=32, ngf=64, ngfs=32, ngpu=1, niter=100, nz=50, outf='./output', print_freq=100, threshold=0.05, w_adv=1, workers=1)

############ signal dataset ############
train_s data size:(62436, 1, 320)
val_s data size:(8025, 1, 320)
test_s N data size:(17343, 1, 320)
test_s S data size:(2723, 1, 320)
test_s V data size:(6307, 1, 320)
test_s F data size:(721, 1, 320)
test_s Q data size:(13, 1, 320)

############ frequency dataset ############
train_f data size:(62436, 1, 128, 128)
val_f data size:(8025, 1, 128, 128)
test_f N data size:(17343, 1, 128, 128)
test_f S data size:(2723, 1, 128, 128)
test_f V data size:(6307, 1, 128, 128)
test_f F data size:(721, 1, 128, 128)
test_f Q data size:(13, 1, 128, 128)
load data success!!!
/data/haenim/lab/multi/experiments/ecg/network.py:25: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  torch.nn.init.xavier_uniform(mod.weight)
Generator(
  (signal_encoder): Signal_Encoder(
    (main): Sequential(
      (0): Conv1d(1, 32, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (1): LeakyReLU(negative_slope=0.2, inplace=True)
      (2): Conv1d(32, 64, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): LeakyReLU(negative_slope=0.2, inplace=True)
      (5): Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.2, inplace=True)
      (8): Conv1d(128, 256, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): LeakyReLU(negative_slope=0.2, inplace=True)
      (11): Conv1d(256, 512, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (12): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): LeakyReLU(negative_slope=0.2, inplace=True)
      (14): Conv1d(512, 50, kernel_size=(10,), stride=(1,), bias=False)
    )
  )
  (freq_2d_encoder): Frequency_2D_Encoder(
    (main): Sequential(
      (0): Conv2d(1, 54, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (1): LeakyReLU(negative_slope=0.2, inplace=True)
      (2): Conv2d(54, 108, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (3): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): LeakyReLU(negative_slope=0.2, inplace=True)
      (5): Conv2d(108, 216, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (6): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.2, inplace=True)
      (8): Conv2d(216, 432, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (9): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): LeakyReLU(negative_slope=0.2, inplace=True)
      (11): Conv2d(432, 864, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)
      (12): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): LeakyReLU(negative_slope=0.2, inplace=True)
      (14): Conv2d(864, 50, kernel_size=(7, 7), stride=(1, 1), bias=False)
    )
  )
  (linear_s): Linear(in_features=1, out_features=128, bias=True)
  (linear_f): Linear(in_features=1, out_features=128, bias=True)
  (linear1): Linear(in_features=100, out_features=50, bias=True)
  (linear2): Linear(in_features=1, out_features=64, bias=True)
  (linear3): Linear(in_features=64, out_features=128, bias=True)
  (tf): Multimodal_Transformer(
    (encoder_layers1): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)
      )
      (linear1): Linear(in_features=50, out_features=512, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (linear2): Linear(in_features=512, out_features=50, bias=True)
      (norm1): LayerNorm((128, 50), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((128, 50), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
    )
    (encoder_layers2): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)
      )
      (linear1): Linear(in_features=50, out_features=512, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (linear2): Linear(in_features=512, out_features=50, bias=True)
      (norm1): LayerNorm((128, 50), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((128, 50), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
    )
    (encoder_layers3): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)
      )
      (linear1): Linear(in_features=50, out_features=512, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (linear2): Linear(in_features=512, out_features=50, bias=True)
      (norm1): LayerNorm((128, 50), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((128, 50), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
    )
  )
  (signal_decoder): Signal_Decoder(
    (main): Sequential(
      (0): ConvTranspose1d(50, 512, kernel_size=(10,), stride=(1,), bias=False)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): ConvTranspose1d(512, 256, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): ConvTranspose1d(256, 128, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): ConvTranspose1d(128, 64, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (13): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): ConvTranspose1d(32, 1, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (16): Tanh()
    )
  )
)
Total number of parameters: 12237920
Discriminator(
  (features_s): Sequential(
    (0): Conv1d(1, 32, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv1d(32, 64, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
    (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): LeakyReLU(negative_slope=0.2, inplace=True)
    (5): Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
    (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): LeakyReLU(negative_slope=0.2, inplace=True)
    (8): Conv1d(128, 256, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): LeakyReLU(negative_slope=0.2, inplace=True)
    (11): Conv1d(256, 512, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
    (12): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (classifier_s): Sequential(
    (0): Conv1d(512, 1, kernel_size=(10,), stride=(1,), bias=False)
    (Sigmoid): Sigmoid()
  )
)
Total number of parameters: 703488

model_device: cuda:0 

################  Train  ##################
Train model.
Epoch: [1] [ 100/1951] D_loss_s(r/f): 0.004762/0.008271, D_loss_f(r/f): 0.004762/0.008271, D_loss(s/f): 0.013033/0.013033, G_loss(s/f): 0.438063/0.438063
Epoch: [1] [ 200/1951] D_loss_s(r/f): 0.003791/0.005046, D_loss_f(r/f): 0.003791/0.005046, D_loss(s/f): 0.008837/0.008837, G_loss(s/f): 0.476455/0.476455
Epoch: [1] [ 300/1951] D_loss_s(r/f): 0.000511/0.000862, D_loss_f(r/f): 0.000511/0.000862, D_loss(s/f): 0.001373/0.001373, G_loss(s/f): 0.468115/0.468115
Epoch: [1] [ 400/1951] D_loss_s(r/f): 0.000551/0.000442, D_loss_f(r/f): 0.000551/0.000442, D_loss(s/f): 0.000993/0.000993, G_loss(s/f): 0.491706/0.491706
Epoch: [1] [ 500/1951] D_loss_s(r/f): 0.000260/0.000308, D_loss_f(r/f): 0.000260/0.000308, D_loss(s/f): 0.000568/0.000568, G_loss(s/f): 0.521845/0.521845
Epoch: [1] [ 600/1951] D_loss_s(r/f): 0.129059/0.079454, D_loss_f(r/f): 0.129059/0.079454, D_loss(s/f): 0.208513/0.208513, G_loss(s/f): 0.221873/0.221873
Epoch: [1] [ 700/1951] D_loss_s(r/f): 0.059696/0.052331, D_loss_f(r/f): 0.059696/0.052331, D_loss(s/f): 0.112027/0.112027, G_loss(s/f): 0.208223/0.208223
Epoch: [1] [ 800/1951] D_loss_s(r/f): 0.018482/0.024531, D_loss_f(r/f): 0.018482/0.024531, D_loss(s/f): 0.043013/0.043013, G_loss(s/f): 0.254064/0.254064
Epoch: [1] [ 900/1951] D_loss_s(r/f): 0.000848/2.180250, D_loss_f(r/f): 0.000848/2.180250, D_loss(s/f): 2.181097/2.181097, G_loss(s/f): 0.170787/0.170787
Epoch: [1] [1000/1951] D_loss_s(r/f): 0.007075/0.050693, D_loss_f(r/f): 0.007075/0.050693, D_loss(s/f): 0.057767/0.057767, G_loss(s/f): 0.284187/0.284187
Epoch: [1] [1100/1951] D_loss_s(r/f): 0.036314/0.002908, D_loss_f(r/f): 0.036314/0.002908, D_loss(s/f): 0.039222/0.039222, G_loss(s/f): 0.299965/0.299965
Epoch: [1] [1200/1951] D_loss_s(r/f): 0.005351/0.000208, D_loss_f(r/f): 0.005351/0.000208, D_loss(s/f): 0.005559/0.005559, G_loss(s/f): 0.367729/0.367729
Epoch: [1] [1300/1951] D_loss_s(r/f): 0.389543/0.040811, D_loss_f(r/f): 0.389543/0.040811, D_loss(s/f): 0.430354/0.430354, G_loss(s/f): 0.153755/0.153755
Epoch: [1] [1400/1951] D_loss_s(r/f): 0.338110/0.004986, D_loss_f(r/f): 0.338110/0.004986, D_loss(s/f): 0.343096/0.343096, G_loss(s/f): 0.173225/0.173225
Epoch: [1] [1500/1951] D_loss_s(r/f): 0.009704/0.011387, D_loss_f(r/f): 0.009704/0.011387, D_loss(s/f): 0.021091/0.021091, G_loss(s/f): 0.258983/0.258983
Epoch: [1] [1600/1951] D_loss_s(r/f): 0.001524/0.003862, D_loss_f(r/f): 0.001524/0.003862, D_loss(s/f): 0.005386/0.005386, G_loss(s/f): 0.323541/0.323541
Epoch: [1] [1700/1951] D_loss_s(r/f): 0.001932/0.001517, D_loss_f(r/f): 0.001932/0.001517, D_loss(s/f): 0.003449/0.003449, G_loss(s/f): 0.322249/0.322249
Epoch: [1] [1800/1951] D_loss_s(r/f): 0.004313/0.007254, D_loss_f(r/f): 0.004313/0.007254, D_loss(s/f): 0.011567/0.011567, G_loss(s/f): 0.276488/0.276488
Epoch: [1] [1900/1951] D_loss_s(r/f): 0.018546/0.004880, D_loss_f(r/f): 0.018546/0.004880, D_loss(s/f): 0.023426/0.023426, G_loss(s/f): 0.231582/0.231582
[1] auc_s:0.8922 th_s:0.0339 f1_s:0.5949 	 best_auc:0.8922 in epoch[1]

[1] auc_f:0.8922 th_f:0.0339 f1_f:0.5949 	 best_auc:0.8922 in epoch[1]

Epoch: [2] [ 100/1951] D_loss_s(r/f): 0.070560/0.002436, D_loss_f(r/f): 0.070560/0.002436, D_loss(s/f): 0.072997/0.072997, G_loss(s/f): 0.305258/0.305258
Epoch: [2] [ 200/1951] D_loss_s(r/f): 0.009313/0.000031, D_loss_f(r/f): 0.009313/0.000031, D_loss(s/f): 0.009344/0.009344, G_loss(s/f): 0.363372/0.363372
Epoch: [2] [ 300/1951] D_loss_s(r/f): 0.004890/0.000527, D_loss_f(r/f): 0.004890/0.000527, D_loss(s/f): 0.005416/0.005416, G_loss(s/f): 0.321100/0.321100
Epoch: [2] [ 400/1951] D_loss_s(r/f): 0.000976/0.007804, D_loss_f(r/f): 0.000976/0.007804, D_loss(s/f): 0.008779/0.008779, G_loss(s/f): 0.285914/0.285914
Epoch: [2] [ 500/1951] D_loss_s(r/f): 0.001502/0.001121, D_loss_f(r/f): 0.001502/0.001121, D_loss(s/f): 0.002623/0.002623, G_loss(s/f): 0.322252/0.322252
Epoch: [2] [ 600/1951] D_loss_s(r/f): 0.000252/0.008492, D_loss_f(r/f): 0.000252/0.008492, D_loss(s/f): 0.008744/0.008744, G_loss(s/f): 0.270234/0.270234
Epoch: [2] [ 700/1951] D_loss_s(r/f): 0.001110/0.000267, D_loss_f(r/f): 0.001110/0.000267, D_loss(s/f): 0.001377/0.001377, G_loss(s/f): 0.314382/0.314382
Epoch: [2] [ 800/1951] D_loss_s(r/f): 0.001552/0.004263, D_loss_f(r/f): 0.001552/0.004263, D_loss(s/f): 0.005815/0.005815, G_loss(s/f): 0.277194/0.277194
Epoch: [2] [ 900/1951] D_loss_s(r/f): 0.001773/0.000861, D_loss_f(r/f): 0.001773/0.000861, D_loss(s/f): 0.002634/0.002634, G_loss(s/f): 0.373868/0.373868
Epoch: [2] [1000/1951] D_loss_s(r/f): 0.174126/0.202447, D_loss_f(r/f): 0.174126/0.202447, D_loss(s/f): 0.376573/0.376573, G_loss(s/f): 0.121976/0.121976

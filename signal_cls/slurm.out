#################################
########  Folder 0  ############
Namespace(batchsize=32, beta1=0.5, dataroot='../../../dataset', dataset='ecg', device='gpu', folder=0, gpu_ids='1', isize=320, istest=False, lr=0.0001, model='beatgan', n_aug=0, name='beatgan/ecg', nc=1, ndf=32, ngf=32, ngpu=1, niter=100, nz=50, outf='./output', print_freq=100, threshold=0.05, w_adv=1, workers=1)
train data size:(62436, 1, 320)
val data size:(8025, 1, 320)
test N data size:(17343, 1, 320)
test S data size:(2723, 1, 320)
test V data size:(6307, 1, 320)
test F data size:(721, 1, 320)
test Q data size:(13, 1, 320)
load data success!!!
/data/haenim/lab/signal_cls/experiments/ecg/network.py:26: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  torch.nn.init.xavier_uniform(mod.weight)
Generator(
  (encoder1): Encoder(
    (main): Sequential(
      (0): Conv1d(1, 32, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (1): LeakyReLU(negative_slope=0.2, inplace=True)
      (2): Conv1d(32, 64, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): LeakyReLU(negative_slope=0.2, inplace=True)
      (5): Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.2, inplace=True)
      (8): Conv1d(128, 256, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): LeakyReLU(negative_slope=0.2, inplace=True)
      (11): Conv1d(256, 512, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (12): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): LeakyReLU(negative_slope=0.2, inplace=True)
      (14): Conv1d(512, 50, kernel_size=(10,), stride=(1,), bias=False)
    )
  )
  (tf): Generator_Transformer(
    (linear1): Linear(in_features=1, out_features=128, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)
          )
          (linear1): Linear(in_features=50, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=50, bias=True)
          (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)
          )
          (linear1): Linear(in_features=50, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=50, bias=True)
          (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)
          )
          (linear1): Linear(in_features=50, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=50, bias=True)
          (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (linear2): Linear(in_features=128, out_features=1, bias=True)
  )
  (decoder): Decoder(
    (main): Sequential(
      (0): ConvTranspose1d(50, 512, kernel_size=(10,), stride=(1,), bias=False)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): ConvTranspose1d(512, 256, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): ConvTranspose1d(256, 128, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): ConvTranspose1d(128, 64, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (13): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): ConvTranspose1d(32, 1, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (16): Tanh()
    )
  )
)
Total number of parameters: 2095671
Discriminator(
  (features): Sequential(
    (0): Conv1d(1, 32, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv1d(32, 64, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
    (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): LeakyReLU(negative_slope=0.2, inplace=True)
    (5): Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
    (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): LeakyReLU(negative_slope=0.2, inplace=True)
    (8): Conv1d(128, 256, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): LeakyReLU(negative_slope=0.2, inplace=True)
    (11): Conv1d(256, 512, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
    (12): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (classifier): Sequential(
    (0): Conv1d(512, 1, kernel_size=(10,), stride=(1,), bias=False)
    (Sigmoid): Sigmoid()
  )
)
Total number of parameters: 703488
################  Train  ##################
Train model.
Epoch: [1] [ 100/1951] D_loss(R/F): 0.292616/0.023594, G_loss: 0.286687
Epoch: [1] [ 200/1951] D_loss(R/F): 0.041734/0.088133, G_loss: 0.244365
Epoch: [1] [ 300/1951] D_loss(R/F): 0.012840/0.043788, G_loss: 0.282061
Epoch: [1] [ 400/1951] D_loss(R/F): 0.033984/0.022713, G_loss: 0.242667
Epoch: [1] [ 500/1951] D_loss(R/F): 0.009394/0.009078, G_loss: 0.373880
Epoch: [1] [ 600/1951] D_loss(R/F): 0.035601/0.009920, G_loss: 0.282812
Epoch: [1] [ 700/1951] D_loss(R/F): 0.010586/0.004333, G_loss: 0.254379
Epoch: [1] [ 800/1951] D_loss(R/F): 0.043960/0.000627, G_loss: 0.371125
Epoch: [1] [ 900/1951] D_loss(R/F): 0.014969/0.008720, G_loss: 0.264789
Epoch: [1] [1000/1951] D_loss(R/F): 0.358325/0.032062, G_loss: 0.169158
Epoch: [1] [1100/1951] D_loss(R/F): 0.021162/0.025752, G_loss: 0.272465
Epoch: [1] [1200/1951] D_loss(R/F): 0.026125/0.002373, G_loss: 0.260371
Epoch: [1] [1300/1951] D_loss(R/F): 0.003703/0.011544, G_loss: 0.224384
Epoch: [1] [1400/1951] D_loss(R/F): 0.767867/0.110104, G_loss: 0.086548
Epoch: [1] [1500/1951] D_loss(R/F): 0.028589/0.024511, G_loss: 0.235654
Epoch: [1] [1600/1951] D_loss(R/F): 0.010772/0.010370, G_loss: 0.230581
Epoch: [1] [1700/1951] D_loss(R/F): 0.011619/0.002261, G_loss: 0.234693
Epoch: [1] [1800/1951] D_loss(R/F): 0.003272/0.000994, G_loss: 0.284183
Epoch: [1] [1900/1951] D_loss(R/F): 0.004894/0.002936, G_loss: 0.245305
[1] auc:0.8843 th:0.0256 f1:0.5964 	 best_auc:0.8843 in epoch[1]

Epoch: [2] [ 100/1951] D_loss(R/F): 0.003194/0.010065, G_loss: 0.244854
Epoch: [2] [ 200/1951] D_loss(R/F): 0.001707/0.013769, G_loss: 0.248810
Epoch: [2] [ 300/1951] D_loss(R/F): 0.003336/0.000085, G_loss: 0.311599
Epoch: [2] [ 400/1951] D_loss(R/F): 0.011955/0.000096, G_loss: 0.375872
Epoch: [2] [ 500/1951] D_loss(R/F): 0.002464/0.004168, G_loss: 0.245328
Epoch: [2] [ 600/1951] D_loss(R/F): 0.621030/0.132265, G_loss: 0.081816
Epoch: [2] [ 700/1951] D_loss(R/F): 0.029652/0.202836, G_loss: 0.190614
Epoch: [2] [ 800/1951] D_loss(R/F): 0.640741/0.016823, G_loss: 0.120003
Epoch: [2] [ 900/1951] D_loss(R/F): 0.006268/0.018413, G_loss: 0.230257
Epoch: [2] [1000/1951] D_loss(R/F): 0.005245/0.032243, G_loss: 0.247662
Epoch: [2] [1100/1951] D_loss(R/F): 0.004660/0.023016, G_loss: 0.254326
Epoch: [2] [1200/1951] D_loss(R/F): 0.007220/0.031019, G_loss: 0.217152
Epoch: [2] [1300/1951] D_loss(R/F): 0.005926/0.002028, G_loss: 0.295600
Epoch: [2] [1400/1951] D_loss(R/F): 0.013684/0.015242, G_loss: 0.215733
Epoch: [2] [1500/1951] D_loss(R/F): 0.002030/0.003086, G_loss: 0.261924
Epoch: [2] [1600/1951] D_loss(R/F): 0.000910/0.007881, G_loss: 0.337427
Epoch: [2] [1700/1951] D_loss(R/F): 0.000960/0.002113, G_loss: 0.315465
Epoch: [2] [1800/1951] D_loss(R/F): 0.001922/0.003411, G_loss: 0.269320
Epoch: [2] [1900/1951] D_loss(R/F): 0.001042/0.000858, G_loss: 0.330184
[2] auc:0.8867 th:0.0484 f1:0.6024 	 best_auc:0.8867 in epoch[2]

Epoch: [3] [ 100/1951] D_loss(R/F): 0.000278/0.000326, G_loss: 0.348171
Epoch: [3] [ 200/1951] D_loss(R/F): 0.000505/0.000562, G_loss: 0.289283
Epoch: [3] [ 300/1951] D_loss(R/F): 0.666162/0.297913, G_loss: 0.036466
Epoch: [3] [ 400/1951] D_loss(R/F): 0.531390/0.103799, G_loss: 0.077831
Epoch: [3] [ 500/1951] D_loss(R/F): 0.090607/0.820613, G_loss: 0.068616
Epoch: [3] [ 600/1951] D_loss(R/F): 0.118061/0.377869, G_loss: 0.111003
Epoch: [3] [ 700/1951] D_loss(R/F): 0.171719/0.191800, G_loss: 0.104558
Epoch: [3] [ 800/1951] D_loss(R/F): 0.098311/0.224542, G_loss: 0.119385
Epoch: [3] [ 900/1951] D_loss(R/F): 0.015223/0.345557, G_loss: 0.150060
Epoch: [3] [1000/1951] D_loss(R/F): 0.016364/0.057784, G_loss: 0.174315
Epoch: [3] [1100/1951] D_loss(R/F): 0.029292/0.007863, G_loss: 0.176509
Epoch: [3] [1200/1951] D_loss(R/F): 0.010102/0.023919, G_loss: 0.178054
Epoch: [3] [1300/1951] D_loss(R/F): 0.000934/0.000959, G_loss: 0.314659
Epoch: [3] [1400/1951] D_loss(R/F): 0.002168/0.007698, G_loss: 0.210029
Epoch: [3] [1500/1951] D_loss(R/F): 0.000770/0.001340, G_loss: 0.220650
Epoch: [3] [1600/1951] D_loss(R/F): 0.003256/0.000257, G_loss: 0.292607
Epoch: [3] [1700/1951] D_loss(R/F): 0.002608/0.000104, G_loss: 0.250211
Epoch: [3] [1800/1951] D_loss(R/F): 0.007425/0.000112, G_loss: 0.290214
Epoch: [3] [1900/1951] D_loss(R/F): 0.002624/0.042924, G_loss: 0.270567
[3] auc:0.8865 th:0.0331 f1:0.6318 	 best_auc:0.8867 in epoch[2]

Epoch: [4] [ 100/1951] D_loss(R/F): 0.000817/0.002147, G_loss: 0.244239
Epoch: [4] [ 200/1951] D_loss(R/F): 0.000966/0.000546, G_loss: 0.276205
Epoch: [4] [ 300/1951] D_loss(R/F): 0.001047/0.000368, G_loss: 0.250419
Epoch: [4] [ 400/1951] D_loss(R/F): 0.000702/0.006005, G_loss: 0.254367
Epoch: [4] [ 500/1951] D_loss(R/F): 0.000209/0.001017, G_loss: 0.323545
Epoch: [4] [ 600/1951] D_loss(R/F): 0.198884/0.236352, G_loss: 0.067076
Epoch: [4] [ 700/1951] D_loss(R/F): 0.017431/0.002717, G_loss: 0.193708
Epoch: [4] [ 800/1951] D_loss(R/F): 0.003734/0.000763, G_loss: 0.217330
Epoch: [4] [ 900/1951] D_loss(R/F): 0.000881/0.035332, G_loss: 0.244421
Epoch: [4] [1000/1951] D_loss(R/F): 0.001074/0.002871, G_loss: 0.242622
Epoch: [4] [1100/1951] D_loss(R/F): 0.000520/0.004005, G_loss: 0.251700

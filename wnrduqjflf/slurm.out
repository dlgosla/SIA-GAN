#################################
########  Folder 0  ############
Namespace(batchsize=64, beta1=0.5, dataroot='../../../dataset', dataset='ecg', device='gpu', folder=0, gpu_ids='1', isize=320, istest=False, lr=0.0001, model='beatgan', n_aug=0, name='beatgan/ecg', nc=1, ndf=32, ngf=32, ngpu=1, niter=100, nz=50, outf='./output', print_freq=100, threshold=0.05, w_adv=1, workers=1)
train data size:(62436, 1, 320)
val data size:(8025, 1, 320)
test N data size:(17343, 1, 320)
test S data size:(2723, 1, 320)
test V data size:(6307, 1, 320)
test F data size:(721, 1, 320)
test Q data size:(13, 1, 320)
load data success!!!
/data/haenim/lab/signal_cls/experiments/ecg/network.py:26: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  torch.nn.init.xavier_uniform(mod.weight)
Generator(
  (encoder1): Encoder(
    (main): Sequential(
      (0): Conv1d(1, 32, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (1): LeakyReLU(negative_slope=0.2, inplace=True)
      (2): Conv1d(32, 64, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): LeakyReLU(negative_slope=0.2, inplace=True)
      (5): Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.2, inplace=True)
      (8): Conv1d(128, 256, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): LeakyReLU(negative_slope=0.2, inplace=True)
      (11): Conv1d(256, 512, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (12): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): LeakyReLU(negative_slope=0.2, inplace=True)
      (14): Conv1d(512, 50, kernel_size=(10,), stride=(1,), bias=False)
    )
  )
  (tf): Generator_Transformer(
    (linear1): Linear(in_features=1, out_features=128, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)
          )
          (linear1): Linear(in_features=50, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=50, bias=True)
          (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)
          )
          (linear1): Linear(in_features=50, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=50, bias=True)
          (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)
          )
          (linear1): Linear(in_features=50, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=50, bias=True)
          (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (linear2): Linear(in_features=128, out_features=1, bias=True)
  )
  (decoder): Decoder(
    (main): Sequential(
      (0): ConvTranspose1d(50, 512, kernel_size=(10,), stride=(1,), bias=False)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): ConvTranspose1d(512, 256, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): ConvTranspose1d(256, 128, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): ConvTranspose1d(128, 64, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (13): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): ConvTranspose1d(32, 1, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (16): Tanh()
    )
  )
)
Total number of parameters: 2095671
Discriminator(
  (features): Sequential(
    (0): Conv1d(1, 32, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv1d(32, 64, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
    (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): LeakyReLU(negative_slope=0.2, inplace=True)
    (5): Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
    (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): LeakyReLU(negative_slope=0.2, inplace=True)
    (8): Conv1d(128, 256, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): LeakyReLU(negative_slope=0.2, inplace=True)
    (11): Conv1d(256, 512, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
    (12): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (classifier): Sequential(
    (0): Conv1d(512, 1, kernel_size=(10,), stride=(1,), bias=False)
    (Sigmoid): Sigmoid()
  )
)
Total number of parameters: 703488
################  Train  ##################
Train model.
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    model.train()
  File "/data/haenim/lab/signal_cls/experiments/ecg/model.py", line 142, in train
    self.train_epoch()
  File "/data/haenim/lab/signal_cls/experiments/ecg/model.py", line 182, in train_epoch
    self.optimize()
  File "/data/haenim/lab/signal_cls/experiments/ecg/model.py", line 227, in optimize
    self.update_netd()
  File "/data/haenim/lab/signal_cls/experiments/ecg/model.py", line 245, in update_netd
    self.fake, self.latent_i = self.G(self.input)
  File "/data/haenim/anaconda3/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/haenim/lab/signal_cls/experiments/ecg/model.py", line 51, in forward
    gen_x = self.decoder(tf)
  File "/data/haenim/anaconda3/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/haenim/lab/signal_cls/experiments/ecg/network.py", line 130, in forward
    output = self.main(input)
  File "/data/haenim/anaconda3/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/haenim/anaconda3/envs/venv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/data/haenim/anaconda3/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/haenim/anaconda3/envs/venv/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 767, in forward
    return F.conv_transpose1d(
RuntimeError: Given transposed=1, weight of size [50, 512, 10], expected input[128, 64, 1] to have 50 channels, but got 64 channels instead

#################################
########  Folder 0  ############
device:  cuda:0
Namespace(batchsize=32, beta1=0.5, dataroot='../../../dataset', dataset='ecg', device='gpu', folder=0, gpu_ids=[2], isize=128, istest=False, lr=0.0001, model='beatgan', n_aug=0, name='beatgan/ecg', nc=1, ndf=64, ndfs=64, ngf=64, ngfs=64, ngpu=1, niter=100, nz=50, outf='./output', print_freq=100, threshold=0.05, w_adv=1, workers=1)

############ signal dataset ############
train_s data size:(62436, 1, 320)
val_s data size:(8025, 1, 320)
test_s N data size:(17343, 1, 320)
test_s S data size:(2723, 1, 320)
test_s V data size:(6307, 1, 320)
test_s F data size:(721, 1, 320)
test_s Q data size:(13, 1, 320)

############ frequency dataset ############
train_f data size:(62436, 1, 128, 128)
val_f data size:(8025, 1, 128, 128)
test_f N data size:(17343, 1, 128, 128)
test_f S data size:(2723, 1, 128, 128)
test_f V data size:(6307, 1, 128, 128)
test_f F data size:(721, 1, 128, 128)
test_f Q data size:(13, 1, 128, 128)
load data success!!!
/data/haenim/lab/trained_linear/experiments/ecg/network.py:25: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  torch.nn.init.xavier_uniform(mod.weight)
Generator(
  (signal_encoder): Signal_Encoder(
    (main): Sequential(
      (0): Conv1d(1, 64, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (1): LeakyReLU(negative_slope=0.2, inplace=True)
      (2): Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): LeakyReLU(negative_slope=0.2, inplace=True)
      (5): Conv1d(128, 256, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (6): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.2, inplace=True)
      (8): Conv1d(256, 512, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): LeakyReLU(negative_slope=0.2, inplace=True)
      (11): Conv1d(512, 1024, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (12): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): LeakyReLU(negative_slope=0.2, inplace=True)
      (14): Conv1d(1024, 50, kernel_size=(10,), stride=(1,), bias=False)
    )
  )
  (freq_2d_encoder): Frequency_2D_Encoder(
    (main): Sequential(
      (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (1): LeakyReLU(negative_slope=0.2, inplace=True)
      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): LeakyReLU(negative_slope=0.2, inplace=True)
      (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): LeakyReLU(negative_slope=0.2, inplace=True)
      (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): LeakyReLU(negative_slope=0.2, inplace=True)
      (11): Conv2d(512, 1024, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)
      (12): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): LeakyReLU(negative_slope=0.2, inplace=True)
      (14): Conv2d(1024, 50, kernel_size=(7, 7), stride=(1, 1), bias=False)
    )
  )
  (linear_s): Linear(in_features=1, out_features=128, bias=True)
  (linear_f): Linear(in_features=1, out_features=128, bias=True)
  (linear1): Linear(in_features=100, out_features=50, bias=True)
  (linear2): Linear(in_features=1, out_features=64, bias=True)
  (linear3): Linear(in_features=64, out_features=128, bias=True)
  (tf): Multimodal_Transformer(
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)
          )
          (linear1): Linear(in_features=50, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=50, bias=True)
          (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)
          )
          (linear1): Linear(in_features=50, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=50, bias=True)
          (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)
          )
          (linear1): Linear(in_features=50, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=50, bias=True)
          (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (signal_decoder): Signal_Decoder(
    (main): Sequential(
      (0): ConvTranspose1d(50, 1024, kernel_size=(10,), stride=(1,), bias=False)
      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): ConvTranspose1d(1024, 512, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): ConvTranspose1d(512, 256, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): ConvTranspose1d(256, 128, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): ConvTranspose1d(128, 64, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (13): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): ConvTranspose1d(64, 1, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)
      (16): Tanh()
    )
  )
)
Total number of parameters: 20458160
signal_encoder.main.0.weight
signal_encoder.main.2.weight
signal_encoder.main.3.weight
signal_encoder.main.3.bias
signal_encoder.main.3.running_mean
signal_encoder.main.3.running_var
signal_encoder.main.3.num_batches_tracked
signal_encoder.main.5.weight
signal_encoder.main.6.weight
signal_encoder.main.6.bias
signal_encoder.main.6.running_mean
signal_encoder.main.6.running_var
signal_encoder.main.6.num_batches_tracked
signal_encoder.main.8.weight
signal_encoder.main.9.weight
signal_encoder.main.9.bias
signal_encoder.main.9.running_mean
signal_encoder.main.9.running_var
signal_encoder.main.9.num_batches_tracked
signal_encoder.main.11.weight
signal_encoder.main.12.weight
signal_encoder.main.12.bias
signal_encoder.main.12.running_mean
signal_encoder.main.12.running_var
signal_encoder.main.12.num_batches_tracked
signal_encoder.main.14.weight
freq_2d_encoder.main.0.weight
freq_2d_encoder.main.2.weight
freq_2d_encoder.main.3.weight
freq_2d_encoder.main.3.bias
freq_2d_encoder.main.3.running_mean
freq_2d_encoder.main.3.running_var
freq_2d_encoder.main.3.num_batches_tracked
freq_2d_encoder.main.5.weight
freq_2d_encoder.main.6.weight
freq_2d_encoder.main.6.bias
freq_2d_encoder.main.6.running_mean
freq_2d_encoder.main.6.running_var
freq_2d_encoder.main.6.num_batches_tracked
freq_2d_encoder.main.8.weight
freq_2d_encoder.main.9.weight
freq_2d_encoder.main.9.bias
freq_2d_encoder.main.9.running_mean
freq_2d_encoder.main.9.running_var
freq_2d_encoder.main.9.num_batches_tracked
freq_2d_encoder.main.11.weight
freq_2d_encoder.main.12.weight
freq_2d_encoder.main.12.bias
freq_2d_encoder.main.12.running_mean
freq_2d_encoder.main.12.running_var
freq_2d_encoder.main.12.num_batches_tracked
freq_2d_encoder.main.14.weight
linear_s.weight
linear_s.bias
linear_f.weight
linear_f.bias
linear1.weight
linear1.bias
linear2.weight
linear2.bias
linear3.weight
linear3.bias
tf.transformer_encoder.layers.0.self_attn.in_proj_weight
tf.transformer_encoder.layers.0.self_attn.in_proj_bias
tf.transformer_encoder.layers.0.self_attn.out_proj.weight
tf.transformer_encoder.layers.0.self_attn.out_proj.bias
tf.transformer_encoder.layers.0.linear1.weight
tf.transformer_encoder.layers.0.linear1.bias
tf.transformer_encoder.layers.0.linear2.weight
tf.transformer_encoder.layers.0.linear2.bias
tf.transformer_encoder.layers.0.norm1.weight
tf.transformer_encoder.layers.0.norm1.bias
tf.transformer_encoder.layers.0.norm2.weight
tf.transformer_encoder.layers.0.norm2.bias
tf.transformer_encoder.layers.1.self_attn.in_proj_weight
tf.transformer_encoder.layers.1.self_attn.in_proj_bias
tf.transformer_encoder.layers.1.self_attn.out_proj.weight
tf.transformer_encoder.layers.1.self_attn.out_proj.bias
tf.transformer_encoder.layers.1.linear1.weight
tf.transformer_encoder.layers.1.linear1.bias
tf.transformer_encoder.layers.1.linear2.weight
tf.transformer_encoder.layers.1.linear2.bias
tf.transformer_encoder.layers.1.norm1.weight
tf.transformer_encoder.layers.1.norm1.bias
tf.transformer_encoder.layers.1.norm2.weight
tf.transformer_encoder.layers.1.norm2.bias
tf.transformer_encoder.layers.2.self_attn.in_proj_weight
tf.transformer_encoder.layers.2.self_attn.in_proj_bias
tf.transformer_encoder.layers.2.self_attn.out_proj.weight
tf.transformer_encoder.layers.2.self_attn.out_proj.bias
tf.transformer_encoder.layers.2.linear1.weight
tf.transformer_encoder.layers.2.linear1.bias
tf.transformer_encoder.layers.2.linear2.weight
tf.transformer_encoder.layers.2.linear2.bias
tf.transformer_encoder.layers.2.norm1.weight
tf.transformer_encoder.layers.2.norm1.bias
tf.transformer_encoder.layers.2.norm2.weight
tf.transformer_encoder.layers.2.norm2.bias
signal_decoder.main.0.weight
signal_decoder.main.1.weight
signal_decoder.main.1.bias
signal_decoder.main.1.running_mean
signal_decoder.main.1.running_var
signal_decoder.main.1.num_batches_tracked
signal_decoder.main.3.weight
signal_decoder.main.4.weight
signal_decoder.main.4.bias
signal_decoder.main.4.running_mean
signal_decoder.main.4.running_var
signal_decoder.main.4.num_batches_tracked
signal_decoder.main.6.weight
signal_decoder.main.7.weight
signal_decoder.main.7.bias
signal_decoder.main.7.running_mean
signal_decoder.main.7.running_var
signal_decoder.main.7.num_batches_tracked
signal_decoder.main.9.weight
signal_decoder.main.10.weight
signal_decoder.main.10.bias
signal_decoder.main.10.running_mean
signal_decoder.main.10.running_var
signal_decoder.main.10.num_batches_tracked
signal_decoder.main.12.weight
signal_decoder.main.13.weight
signal_decoder.main.13.bias
signal_decoder.main.13.running_mean
signal_decoder.main.13.running_var
signal_decoder.main.13.num_batches_tracked
signal_decoder.main.15.weight
tensor([[ 0.0941],
        [ 0.0196],
        [ 0.1883],
        [ 0.2113],
        [-0.1002],
        [ 0.1115],
        [-0.1933],
        [-0.0076],
        [ 0.0758],
        [ 0.0542],
        [-0.0787],
        [-0.0409],
        [-0.0672],
        [ 0.2150],
        [ 0.0224],
        [-0.2101],
        [-0.0866],
        [ 0.0781],
        [ 0.1099],
        [ 0.1852],
        [ 0.0867],
        [ 0.0082],
        [ 0.0368],
        [-0.1820],
        [-0.1675],
        [ 0.0851],
        [-0.0836],
        [-0.0381],
        [-0.2121],
        [-0.1841],
        [-0.1899],
        [ 0.1736],
        [-0.1671],
        [ 0.1655],
        [ 0.1888],
        [-0.0695],
        [-0.1506],
        [ 0.2149],
        [-0.2036],
        [ 0.1248],
        [-0.0392],
        [-0.0384],
        [ 0.1355],
        [ 0.0372],
        [-0.0962],
        [ 0.0927],
        [ 0.1597],
        [ 0.1888],
        [ 0.0604],
        [ 0.0306],
        [ 0.0565],
        [-0.1846],
        [-0.1703],
        [ 0.0357],
        [-0.1470],
        [ 0.0830],
        [ 0.1566],
        [-0.1488],
        [-0.2033],
        [-0.1983],
        [ 0.1197],
        [ 0.2041],
        [-0.0733],
        [-0.0572],
        [ 0.1629],
        [-0.2073],
        [ 0.1286],
        [-0.1658],
        [ 0.1780],
        [-0.1325],
        [-0.1163],
        [-0.0109],
        [ 0.0404],
        [ 0.1558],
        [-0.0016],
        [-0.0694],
        [ 0.0351],
        [ 0.1165],
        [-0.0021],
        [ 0.1651],
        [ 0.0651],
        [ 0.0372],
        [ 0.0164],
        [ 0.1624],
        [-0.0163],
        [ 0.0266],
        [ 0.1760],
        [-0.0255],
        [-0.0707],
        [ 0.0766],
        [-0.1814],
        [ 0.0289],
        [ 0.2007],
        [ 0.0926],
        [ 0.1321],
        [-0.0578],
        [ 0.1772],
        [-0.2029],
        [-0.1978],
        [-0.0316],
        [-0.1005],
        [ 0.0055],
        [ 0.0357],
        [-0.2017],
        [-0.1586],
        [-0.0279],
        [-0.0370],
        [-0.0907],
        [ 0.0094],
        [-0.1157],
        [-0.1576],
        [ 0.0902],
        [ 0.1264],
        [ 0.2054],
        [ 0.1305],
        [ 0.0793],
        [-0.1374],
        [ 0.1269],
        [ 0.1266],
        [-0.2003],
        [ 0.1208],
        [-0.1372],
        [ 0.1045],
        [ 0.0557],
        [ 0.0716],
        [-0.0914],
        [-0.0343],
        [-0.1116]], device='cuda:0') original
tensor([[-0.0034],
        [-0.2601],
        [-0.2601],
        [-0.2147],
        [ 0.2331],
        [-0.0023],
        [-0.1132],
        [-0.2604],
        [-0.2602],
        [-0.0133],
        [ 0.2233],
        [ 0.2334],
        [-0.2603],
        [-0.2337],
        [ 0.1547],
        [-0.2604],
        [ 0.2341],
        [-0.2603],
        [ 0.1815],
        [ 0.2333],
        [ 0.0138],
        [-0.2604],
        [ 0.0009],
        [ 0.2331],
        [ 0.2331],
        [-0.2604],
        [ 0.2329],
        [ 0.2331],
        [ 0.1239],
        [-0.2202],
        [ 0.2335],
        [ 0.0102],
        [-0.2070],
        [ 0.2331],
        [ 0.2328],
        [ 0.0023],
        [ 0.0017],
        [-0.1232],
        [ 0.2206],
        [ 0.2332],
        [-0.2603],
        [-0.2604],
        [ 0.1516],
        [-0.2601],
        [ 0.0133],
        [ 0.0214],
        [ 0.0110],
        [ 0.2232],
        [ 0.0766],
        [-0.2624]], device='cuda:0')
Traceback (most recent call last):
  File "main.py", line 42, in <module>
    model=MyModel(opt,dataloader,device)
  File "/data/haenim/lab/trained_linear/experiments/ecg/model.py", line 153, in __init__
    self.G.load_state_dict(model_dict, strict=True)
  File "/data/haenim/anaconda3/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1406, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for Generator:
	size mismatch for linear_f.weight: copying a param with shape torch.Size([50, 1]) from checkpoint, the shape in current model is torch.Size([128, 1]).
	size mismatch for linear_f.bias: copying a param with shape torch.Size([50]) from checkpoint, the shape in current model is torch.Size([128]).

#################################
########  Folder 0  ############
device:  cuda:0
Namespace(batchsize=32, beta1=0.5, dataroot='../../../dataset', dataset='ecg', device='gpu', folder=0, gpu_ids=[2], isize=128, istest=False, lr=0.0001, model='beatgan', n_aug=0, name='beatgan/ecg', nc=1, ndf=54, ndfs=32, ngf=64, ngfs=32, ngpu=1, niter=100, nz=50, outf='./output', print_freq=100, threshold=0.05, w_adv=1, workers=1)

############ signal dataset ############
train_s data size:(62436, 1, 320)
val_s data size:(8025, 1, 320)
test_s N data size:(17343, 1, 320)
test_s S data size:(2723, 1, 320)
test_s V data size:(6307, 1, 320)
test_s F data size:(721, 1, 320)
test_s Q data size:(13, 1, 320)

############ frequency dataset ############
train_f data size:(62436, 1, 128, 128)
val_f data size:(8025, 1, 128, 128)
test_f N data size:(17343, 1, 128, 128)
test_f S data size:(2723, 1, 128, 128)
test_f V data size:(6307, 1, 128, 128)
test_f F data size:(721, 1, 128, 128)
test_f Q data size:(13, 1, 128, 128)
load data success!!!
Traceback (most recent call last):
  File "main.py", line 42, in <module>
    model=MyModel(opt,dataloader,device)
  File "/data/haenim/lab/freq/experiments/ecg/model.py", line 78, in __init__
    self.G = Generator( opt).to(device)
  File "/data/haenim/lab/freq/experiments/ecg/model.py", line 55, in __init__
    self.tf = Generator_Transformer(ninp=128, nhead=5, nhid=512, dropout=0.0, nlayers=3)
  File "/data/haenim/lab/freq/experiments/ecg/network.py", line 34, in __init__
    encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout, activation='gelu')
  File "/data/haenim/anaconda3/envs/venv/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 290, in __init__
    self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,
  File "/data/haenim/anaconda3/envs/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 922, in __init__
    assert self.head_dim * num_heads == self.embed_dim, "embed_dim must be divisible by num_heads"
AssertionError: embed_dim must be divisible by num_heads

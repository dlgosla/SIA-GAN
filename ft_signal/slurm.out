#################################
########  Folder 0  ############
device:  cuda:0
Namespace(batchsize=32, beta1=0.5, dataroot='../../../dataset', dataset='ecg', device='gpu', folder=0, gpu_ids=[0], isize=128, istest=True, lr=0.0001, model='beatgan', n_aug=0, name='beatgan/ecg', nc=1, ndf=54, ndfs=32, ngf=64, ngfs=32, ngpu=1, niter=100, nz=50, outf='./output', print_freq=100, threshold=0.169, w_adv=1, workers=1)

############ signal dataset ############
train_s data size:(62436, 1, 320)
val_s data size:(8025, 1, 320)
test_s N data size:(17343, 1, 320)
test_s S data size:(2723, 1, 320)
test_s V data size:(6307, 1, 320)
test_s F data size:(721, 1, 320)
test_s Q data size:(13, 1, 320)

############ frequency dataset ############
train_f data size:(62436, 1, 128, 128)
val_f data size:(8025, 1, 128, 128)
test_f N data size:(17343, 1, 128, 128)
test_f S data size:(2723, 1, 128, 128)
test_f V data size:(6307, 1, 128, 128)
test_f F data size:(721, 1, 128, 128)
test_f Q data size:(13, 1, 128, 128)
load data success!!!
/data/haenim/lab/ft_signal/experiments/ecg/network.py:25: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  torch.nn.init.xavier_uniform(mod.weight)
tensor([ 0.0710,  0.1183,  0.1384, -0.1375], device='cuda:0')
tensor([ 0.0161,  0.2682, -0.0138,  0.0841], device='cuda:0')
------
tensor([ 0.0239,  0.2660, -0.0212,  0.1045], device='cuda:0')
tensor([ 0.0161,  0.2682, -0.0138,  0.0841], device='cuda:0')

model_device: cuda:0 

################  Eval  ##################
############   Analysis   #############
############   Threshold:0.169   #############
*********  Type:S  *************
TP:114
FP:11
TN:17332
FN:2609
Accuracy:0.8694308781022625
Precision/ppv:0.912
sensitivity/Recall:0.04186558942343004
specificity:0.9993657383382345
F1:0.0800561797752809
*********  Type:V  *************
TP:1357
FP:11
TN:17332
FN:4950
Accuracy:0.7902325581395349
Precision/ppv:0.9919590643274854
sensitivity/Recall:0.21515776121769462
specificity:0.9993657383382345
F1:0.3536156351791531
*********  Type:F  *************
TP:6
FP:11
TN:17332
FN:715
Accuracy:0.9598095659875997
Precision/ppv:0.35294117647058826
sensitivity/Recall:0.008321775312066574
specificity:0.9993657383382345
F1:0.016260162601626018
*********  Type:Q  *************
TP:5
FP:11
TN:17332
FN:8
Accuracy:0.9989052777137589
Precision/ppv:0.3125
sensitivity/Recall:0.38461538461538464
specificity:0.9993657383382345
F1:0.3448275862068966
#############################
########  Result  ###########
ap:0.8962402772578691
auc:0.9292142454446415
best th:0.005791463889181614 --> best f1:0.7898698128559805
